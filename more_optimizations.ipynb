{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Respondents</th>\n",
       "      <th>Release Numbers</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>June 2, 2023</td>\n",
       "      <td>Pending Administrative Proceedings (Order Vaca...</td>\n",
       "      <td>33-11199, 34-97641, IA-6324, IC-34934, AAER-4414</td>\n",
       "      <td>https://www.sec.gov/files/litigation/opinions/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>June 2, 2023</td>\n",
       "      <td>Pending Administrative Proceedings (Order Dism...</td>\n",
       "      <td>33-11198, 34-97640, IA-6323, IC-34933, AAER-4413</td>\n",
       "      <td>https://www.sec.gov/files/litigation/opinions/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>June 1, 2023</td>\n",
       "      <td>Carl S. Schwartz, CPA</td>\n",
       "      <td>IA-6320, AAER-4412</td>\n",
       "      <td>https://www.sec.gov/files/litigation/admin/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>May 26, 2023</td>\n",
       "      <td>Gartner, Inc.</td>\n",
       "      <td>34-97609, AAER-4411</td>\n",
       "      <td>https://www.sec.gov/files/litigation/admin/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>May 25, 2023</td>\n",
       "      <td>Jia Roger Qian Wang, CPA and Wang Certified Pu...</td>\n",
       "      <td>34-97590, AAER-4410</td>\n",
       "      <td>https://www.sec.gov/files/litigation/opinions/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date                                        Respondents  \\\n",
       "0  June 2, 2023  Pending Administrative Proceedings (Order Vaca...   \n",
       "1  June 2, 2023  Pending Administrative Proceedings (Order Dism...   \n",
       "2  June 1, 2023                              Carl S. Schwartz, CPA   \n",
       "3  May 26, 2023                                      Gartner, Inc.   \n",
       "4  May 25, 2023  Jia Roger Qian Wang, CPA and Wang Certified Pu...   \n",
       "\n",
       "                                    Release Numbers  \\\n",
       "0  33-11199, 34-97641, IA-6324, IC-34934, AAER-4414   \n",
       "1  33-11198, 34-97640, IA-6323, IC-34933, AAER-4413   \n",
       "2                                IA-6320, AAER-4412   \n",
       "3                               34-97609, AAER-4411   \n",
       "4                               34-97590, AAER-4410   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.sec.gov/files/litigation/opinions/...  \n",
       "1  https://www.sec.gov/files/litigation/opinions/...  \n",
       "2  https://www.sec.gov/files/litigation/admin/202...  \n",
       "3  https://www.sec.gov/files/litigation/admin/202...  \n",
       "4  https://www.sec.gov/files/litigation/opinions/...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import logging\n",
    "import configparser\n",
    "import requests_cache\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Load configuration from a file\n",
    "class ConfigLoader:\n",
    "    @staticmethod\n",
    "    def load_config(filename):\n",
    "        config = configparser.ConfigParser()\n",
    "        with open(filename) as f:\n",
    "            config.read_file(f)\n",
    "        return config\n",
    "\n",
    "\n",
    "class TableScraper:\n",
    "    def __init__(self, page, config):\n",
    "        \"\"\"\n",
    "        Initializes the TableScraper class.\n",
    "\n",
    "        Args:\n",
    "            page (int): The page number to scrape.\n",
    "            config (ConfigParser): The configuration parser object.\n",
    "        \"\"\"\n",
    "        self.base_url = config.get('Scraper', 'base_url')\n",
    "        self.page = page\n",
    "        self.url = self.construct_url()\n",
    "        self.data = []\n",
    "\n",
    "    def construct_url(self):\n",
    "        \"\"\"\n",
    "        Constructs the URL for scraping based on the page number.\n",
    "\n",
    "        Returns:\n",
    "            str: The constructed URL.\n",
    "        \"\"\"\n",
    "        params = {\"page\": self.page}\n",
    "        url = urljoin(self.base_url, \"?\".join([self.base_url, \"&\".join(f\"{k}={v}\" for k, v in params.items())]))\n",
    "        return url\n",
    "\n",
    "    def fetch_html_content(self):\n",
    "        \"\"\"\n",
    "        Fetches the HTML content of the webpage.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(self.url)\n",
    "            response.raise_for_status()  # Raise an exception if the request was not successful\n",
    "            self.html_content = response.content\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error occurred while fetching HTML content: {e}\")\n",
    "\n",
    "    def parse_html_content(self):\n",
    "        \"\"\"\n",
    "        Parses the HTML content using BeautifulSoup.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(self.html_content, 'html.parser')\n",
    "        self.table = soup.find('table', class_='list')\n",
    "\n",
    "    def scrape_table_data(self):\n",
    "        \"\"\"\n",
    "        Scrapes the table data and stores it in a list of dictionaries.\n",
    "        \"\"\"\n",
    "        if self.table is None:\n",
    "            logging.warning(\"No table found on the page.\")\n",
    "            return\n",
    "\n",
    "        rows = self.table.select('.pr-list-page-row')\n",
    "\n",
    "        for row in rows:\n",
    "            self.data.append(self.extract_row_data(row))\n",
    "\n",
    "    def extract_row_data(self, row):\n",
    "        \"\"\"\n",
    "        Extracts data from a table row and returns a dictionary.\n",
    "\n",
    "        Args:\n",
    "            row (BeautifulSoup): The BeautifulSoup object representing a table row.\n",
    "\n",
    "        Returns:\n",
    "            dict: The extracted data as a dictionary.\n",
    "        \"\"\"\n",
    "        date_element = row.select_one('.datetime')\n",
    "        date = date_element.text.strip() if date_element else None\n",
    "\n",
    "        respondents_element = row.select_one('.release-view__respondents')\n",
    "        respondents = respondents_element.text.strip() if respondents_element else None\n",
    "\n",
    "        release_numbers_element = row.select_one('.view-table_subfield.view-table_subfield_release_number')\n",
    "        release_numbers = release_numbers_element.text.replace('Release No.', '').strip() if release_numbers_element else None\n",
    "\n",
    "        link_element = respondents_element.find('a')\n",
    "        link = urljoin(self.base_url, link_element['href']) if link_element else None\n",
    "\n",
    "        return {'Date': date, 'Respondents': respondents, 'Release Numbers': release_numbers, 'Link': link}\n",
    "\n",
    "    def create_dataframe(self):\n",
    "        \"\"\"\n",
    "        Creates a DataFrame from the scraped data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame containing the scraped data.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self.data)\n",
    "        return df\n",
    "\n",
    "    def scrape_and_get_dataframe(self):\n",
    "        \"\"\"\n",
    "        Performs the entire scraping process and returns the DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame containing the scraped data.\n",
    "        \"\"\"\n",
    "        self.session = requests_cache.CachedSession(backend='memory', expire_after=3600)\n",
    "        self.fetch_html_content()\n",
    "        self.parse_html_content()\n",
    "        self.scrape_table_data()\n",
    "        df = self.create_dataframe()\n",
    "        return df\n",
    "\n",
    "\n",
    "# Load configuration from file\n",
    "config = ConfigLoader.load_config('config.ini')\n",
    "\n",
    "# Create an instance of the TableScraper class with page number 0\n",
    "scraper = TableScraper(0, config)\n",
    "\n",
    "# Scrape the table and get the DataFrame\n",
    "df = scraper.scrape_and_get_dataframe()\n",
    "\n",
    "# Print the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
